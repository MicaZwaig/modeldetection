<!DOCTYPE html>
<html lang="es">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Detector de Deepfake — Pro (Local)</title>
  <style>
    :root{
      --bg: #0b1020;
      --card: rgba(255,255,255,0.06);
      --card-strong: rgba(255,255,255,0.12);
      --text: #eaf2ff;
      --muted: #9fb0cc;
      --accent: #7aa2ff;
      --accent-2:#8af0d6;
      --danger:#ff6b6b;
      --success:#00d39b;
      --warning:#ffb86b;
      --shadow: 0 15px 40px rgba(0,0,0,0.35);
      --radius: 16px;
    }
    *{box-sizing:border-box}
    body{
      margin:0; font-family: Inter, ui-sans-serif, system-ui, -apple-system, Segoe UI, Roboto, "Helvetica Neue", Arial;
      color:var(--text);
      background: radial-gradient(1200px 600px at 15% 0%, #151a33 0%, #0b1020 40%, #0b1020 100%);
      min-height:100svh;
    }
    header{ display:flex; align-items:center; justify-content:space-between; gap:12px; padding:24px clamp(16px, 4vw, 40px); }
    .brand{ display:flex; align-items:center; gap:12px; font-weight:700; letter-spacing:.3px; }
    .pill{ padding:.35rem .7rem; border-radius:999px; background:linear-gradient(90deg,var(--accent),var(--accent-2)); color:#0b1020; font-size:.8rem; font-weight:700; }
    .status{ display:flex; gap:8px; align-items:center; flex-wrap:wrap; }
    .badge{ padding:.25rem .55rem; border-radius:999px; border:1px solid var(--card-strong); background:rgba(255,255,255,.06); font-size:.8rem; }
    .container{ max-width:1100px; margin:0 auto; padding:0 clamp(16px, 4vw, 40px) 40px; }
    .grid{ display:grid; grid-template-columns: 1.2fr .8fr; gap:24px; } @media (max-width: 980px){ .grid{ grid-template-columns:1fr; } }
    .card{ background:var(--card); border:1px solid var(--card-strong); border-radius:var(--radius); box-shadow:var(--shadow); backdrop-filter:saturate(120%) blur(6px); }
    .panel{ padding:20px; }
    .title{ font-size:1.25rem; font-weight:700; margin:0 0 6px; }
    .subtitle{ color:var(--muted); margin:0 0 14px; }
    .dropzone{ border:2px dashed rgba(255,255,255,.18); border-radius:14px; padding:22px; display:flex; align-items:center; gap:14px; background:linear-gradient(180deg, rgba(255,255,255,.05), rgba(255,255,255,.02)); transition: .25s ease; }
    .dropzone.drag{ border-color: var(--accent); background: linear-gradient(180deg, rgba(122,162,255,.15), rgba(138,240,214,.08)); }
    .dropzone input{ display:none; }
    .dropzone .dz-label{ flex:1; }
    .dropzone button{ background:var(--accent); color:#0b1020; border:none; border-radius:12px; padding:.75rem 1rem; font-weight:700; cursor:pointer; }
    .actions{ display:flex; gap:10px; flex-wrap:wrap; margin-top:14px; }
    .btn{ padding:.75rem 1rem; border-radius:12px; border:1px solid var(--card-strong); background:rgba(255,255,255,.06); color:var(--text); font-weight:700; cursor:pointer; transition:.2s ease; }
    .btn:hover{ transform: translateY(-1px); background:rgba(255,255,255,.1); }
    .btn.primary{ background:linear-gradient(90deg, var(--accent), var(--accent-2)); color:#0b1020; border:none; }
    .btn.ghost{ background:transparent; }
    .video-wrap{ position:relative; overflow:hidden; border-radius:12px; background:#000; aspect-ratio:16/9; display:grid; place-items:center; }
    video{ width:100%; height:100%; object-fit:contain; background:#000; }
    .progress{ height:10px; width:100%; background:rgba(255,255,255,.08); border-radius:999px; overflow:hidden; }
    .progress .bar{ height:100%; width:0%; background:linear-gradient(90deg, var(--accent), var(--accent-2)); transition: width .2s ease; }
    .result-card{ display:grid; grid-template-columns: 160px 1fr; gap:18px; align-items:center; } @media (max-width: 520px){ .result-card{ grid-template-columns:1fr; } }
    .gauge{ width:160px; height:160px; position:relative; }
    .gauge canvas{ position:absolute; inset:0; }
    .gauge .center{ position:absolute; inset:0; display:grid; place-items:center; text-align:center; }
    .gauge .center .big{ font-size:1.75rem; font-weight:800; }
    .gauge .center .small{ color:var(--muted); font-size:.9rem; }
    .legend{ display:grid; gap:10px; }
    .metric-row{ display:grid; grid-template-columns: 160px 1fr auto; gap:10px; align-items:center; }
    .metric-row .name{ color:var(--muted); font-size:.9rem; }
    .metric-row .bar{ height:10px; background:rgba(255,255,255,.08); border-radius:999px; overflow:hidden; }
    .metric-row .bar > span{ display:block; height:100%; width:0%; background:linear-gradient(90deg, var(--warning), var(--danger)); }
    .metric-row .val{ font-weight:700; }
    .tip{ font-size:.9rem; color:var(--muted); }
    .dot{ width:8px; height:8px; display:inline-block; border-radius:999px; margin-right:6px; }
    footer{ color:#92a3c3; opacity:.9; font-size:.9rem; text-align:center; padding:18px; }
    .kbd{background:rgba(255,255,255,.12); padding:.2rem .45rem; border-radius:6px; border:1px solid rgba(255,255,255,.2);} .danger{ color: var(--danger); } .success{ color: var(--success); }
    .row{ display:flex; gap:10px; align-items:center; flex-wrap:wrap; }
    .mini{ font-size:.8rem; color:var(--muted); }
    .slider{ width:220px; }
  </style>
  <!-- ORT Web (ONNXRuntime) -->
  <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.min.js"></script>
  <!-- MediaPipe Tasks Vision (FaceLandmarker) -->
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.10/wasm/vision_bundle.js"></script>
</head>
<body>
  <header>
    <div class="brand">
      <span class="pill">Pro</span>
      <div>Detector de Deepfake (todo local)</div>
    </div>
    <div class="status">
      <span class="badge" id="status-onnx">Modelo IA: cargando…</span>
      <span class="badge" id="status-face">Landmarks: cargando…</span>
      <span class="badge" id="status-mode">Modo: heurístico+IA</span>
    </div>
  </header>

  <main class="container grid">
    <!-- LEFT: input + preview -->
    <section class="card panel">
      <h2 class="title">1) Cargá un video</h2>
      <p class="subtitle">Se muestrean hasta 5 fps y se analiza el rostro con landmarks + un modelo ONNX (si está disponible) y métricas avanzadas.</p>

      <label class="dropzone" id="dropzone">
        <input id="file" type="file" accept="video/*" />
        <div class="dz-label">
          <strong>Arrastrá y soltá</strong> un video acá, o <span class="kbd">clic</span> para elegir.
          <div class="tip" style="margin-top:6px">Consejo: usar 10–60s con rostro visible. También se analiza el <strong>nombre del archivo</strong>.</div>
        </div>
        <button type="button" id="pickBtn">Elegir archivo</button>
      </label>

      <div class="actions">
        <button class="btn primary" id="analyzeBtn" disabled>Analizar video</button>
        <button class="btn" id="exportBtn" disabled>Exportar reporte (.json)</button>
        <button class="btn ghost" id="resetBtn">Reiniciar</button>
        <div class="row mini">
          Umbral de sospecha:
          <input id="thres" class="slider" type="range" min="30" max="80" value="60"/>
          <span id="thresVal">60%</span>
        </div>
      </div>

      <div style="margin-top:16px" class="video-wrap">
        <video id="video" controls crossorigin="anonymous"></video>
      </div>

      <div style="margin-top:16px" class="progress" aria-label="Progreso de análisis">
        <div class="bar" id="progressBar"></div>
      </div>

      <div style="margin-top:10px" class="tip">
        Este modo usa: <em>modelo IA ONNX (si carga)</em> + <em>blink-rate</em>, <em>sincronía labios–audio</em>, <em>DCT alta frecuencia</em>, <em>consistencia de piel</em>, <em>bloques de compresión</em>, y <em>pistas del nombre del archivo</em>.
      </div>
    </section>

    <!-- RIGHT: results -->
    <aside class="card panel">
      <h2 class="title">2) Resultado</h2>
      <p class="subtitle">Probabilidad estimada de deepfake. Si el modelo IA no está disponible, se usa el modo heurístico reforzado.</p>

      <div class="result-card">
        <div class="gauge">
          <canvas id="gauge" width="160" height="160" aria-label="Porcentaje de sospecha"></canvas>
          <div class="center">
            <div class="big" id="pct">—%</div>
            <div class="small" id="label">Sin análisis</div>
          </div>
        </div>
        <div class="legend" id="legend"></div>
      </div>

      <div style="margin-top:14px" class="tip">
        <strong>Interpretación:</strong>
        <ul style="margin:.4rem 0 .2rem 1.1rem; padding:0">
          <li><span class="success">0–35%</span> auténtico probable</li>
          <li><span class="warning" style="color:var(--warning)">35–65%</span> incierto</li>
          <li><span class="danger">65–100%</span> potencial deepfake</li>
        </ul>
      </div>
    </aside>
  </main>

  <footer>
    Demo educativa. Todo corre localmente. Podés reemplazar el modelo ONNX por otro (Xception/EfficientNet distil) ubicado en <code>CFG.modelUrl</code>.
  </footer>

  <!-- Work canvases -->
  <canvas id="work" width="320" height="180" style="display:none"></canvas>
  <canvas id="face" width="224" height="224" style="display:none"></canvas>
  <canvas id="blur" width="160" height="90" style="display:none"></canvas>

  <script>
  // ================== Config ==================
  const CFG = {
    modelUrl: 'models/df_detector_xceptionlite.onnx', // Cambiá por tu ONNX. Si no existe, el análisis sigue sin IA.
    faceLandmarkerAsset: 'https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/1/face_landmarker.task',
    fps: 5,
    maxFrames: 450,
    capDuration: 120, // segundos
    faceCropScale: 1.3 // escala extra sobre el rostro para incluir contorno
  };

  // ================== UI refs ==================
  const $ = sel => document.querySelector(sel);
  const fileInput = $('#file');
  const dropzone = $('#dropzone');
  const pickBtn = $('#pickBtn');
  const analyzeBtn = $('#analyzeBtn');
  const exportBtn = $('#exportBtn');
  const resetBtn = $('#resetBtn');
  const video = $('#video');
  const bar = $('#progressBar');
  const gaugeCanvas = $('#gauge');
  const pctEl = $('#pct');
  const labelEl = $('#label');
  const legendEl = $('#legend');
  const thres = $('#thres');
  const thresVal = $('#thresVal');
  const stOnnx = $('#status-onnx');
  const stFace = $('#status-face');

  let currentFile = null;
  let lastReport = null;
  let audioBuffer = null;

  thres.addEventListener('input', ()=> thresVal.textContent = thres.value + '%');

  pickBtn.addEventListener('click', () => fileInput.click());
  dropzone.addEventListener('dragover', (e)=>{ e.preventDefault(); dropzone.classList.add('drag'); });
  dropzone.addEventListener('dragleave', ()=> dropzone.classList.remove('drag'));
  dropzone.addEventListener('drop', (e)=>{ e.preventDefault(); dropzone.classList.remove('drag'); const f=e.dataTransfer.files?.[0]; if(f) loadFile(f); });
  fileInput.addEventListener('change', (e)=>{ const f=e.target.files?.[0]; if(f) loadFile(f); });
  resetBtn.addEventListener('click', resetAll);

  function resetAll(){
    video.removeAttribute('src'); video.load(); analyzeBtn.disabled = true; exportBtn.disabled = true; bar.style.width='0%';
    setGauge(0, 'Sin análisis'); legendEl.innerHTML=''; lastReport=null; currentFile=null; audioBuffer=null;
  }

  function loadFile(file){
    currentFile = file; const url = URL.createObjectURL(file);
    video.src = url; video.currentTime = 0; bar.style.width = '0%'; analyzeBtn.disabled = false; exportBtn.disabled = true; setGauge(0, 'Listo para analizar');
    // Pre-decode audio for lipsync metric
    const reader = new FileReader(); reader.onload = async () => { try{ const ac = new (window.AudioContext||window.webkitAudioContext)(); audioBuffer = await ac.decodeAudioData(reader.result); }catch{} }; reader.readAsArrayBuffer(file);
  }

  exportBtn.addEventListener('click', ()=>{ if(!lastReport) return; const blob = new Blob([JSON.stringify(lastReport,null,2)], {type:'application/json'}); const a=document.createElement('a'); a.href=URL.createObjectURL(blob); a.download=`deepfake-reporte-${Date.now()}.json`; a.click(); });

  // ================== IA & Landmarks init ==================
  let session = null; let modelReady=false; let faceReady=false; let FaceLandmarkerRef=null; let FilesetResolverRef=null; let faceLandmarker=null;
  (async function init(){
    try{
      // ONNX Runtime
      session = await ort.InferenceSession.create(CFG.modelUrl, { executionProviders: ['wasm'] });
      modelReady = true; stOnnx.textContent = 'Modelo IA: cargado'; stOnnx.style.borderColor = '#00d39b';
    }catch(e){ stOnnx.textContent = 'Modelo IA: no disponible (heurístico)'; stOnnx.style.borderColor = '#ffb86b'; }

    try{
      FilesetResolverRef = window.FilesetResolver; FaceLandmarkerRef = window.FaceLandmarker;
      const fileset = await FilesetResolverRef.forVisionTasks('https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.10/wasm');
      faceLandmarker = await FaceLandmarkerRef.createFromOptions(fileset, {
        baseOptions: { modelAssetPath: CFG.faceLandmarkerAsset },
        outputFaceBlendshapes: true,
        runningMode: 'IMAGE',
        numFaces: 1
      });
      faceReady = true; stFace.textContent = 'Landmarks: ok'; stFace.style.borderColor = '#00d39b';
    }catch(e){ stFace.textContent = 'Landmarks: no disponibles'; stFace.style.borderColor = '#ff6b6b'; }
  })();

  // ================== Analysis ==================
  const work = $('#work'); const wctx = work.getContext('2d', { willReadFrequently:true });
  const faceC = $('#face'); const fctx = faceC.getContext('2d', { willReadFrequently:true });
  const blurC = $('#blur'); const bctx = blurC.getContext('2d', { willReadFrequently:true });

  const once = (el, ev) => new Promise(r => el.addEventListener(ev, function h(e){ el.removeEventListener(ev, h); r(e); }, {once:true}));

  analyzeBtn.addEventListener('click', async ()=>{
    analyzeBtn.disabled = true; exportBtn.disabled = true; legendEl.innerHTML='';
    try{
      const report = await analyzeVideo(video, (p)=> bar.style.width = p + '%');
      lastReport = report; exportBtn.disabled = true === false; renderReport(report);
    }catch(err){ console.error(err); alert('Error procesando el video: '+ err.message); }
    finally{ analyzeBtn.disabled = false; }
  });

  async function analyzeVideo(video, onProgress){
    await ensureMetadata(video);
    const fps = CFG.fps; const maxFrames=CFG.maxFrames; const duration = Math.min(video.duration||0, CFG.capDuration);
    const {frames, times} = await captureFrames(video, fps, maxFrames, duration, onProgress);
    if(frames.length < 6) throw new Error('Muy pocos cuadros para analizar');

    // Per-frame core stats
    const per = [];
    const earSeries = []; // eye aspect ratio per frame
    const marSeries = []; // mouth aspect ratio per frame
    for(let i=0;i<frames.length;i++){
      const img = frames[i];
      const base = frameStats(img);
      let faceProb = null; let ear=null, mar=null; let facePatch = null;
      if(faceReady){
        const faceInfo = detectFace(img);
        if(faceInfo){ ear = faceInfo.ear; mar = faceInfo.mar; facePatch = faceInfo.patch; }
      }
      let onnxProb = null;
      if(modelReady && facePatch){ onnxProb = await runOnnx(facePatch); }
      const dctHF = facePatch ? dctHighFreq(facePatch) : 0; // 0..1
      const skinVar = facePatch ? skinChromVar(facePatch) : 0; // 0..1 risk
      per.push({ ...base, onnxProb, dctHF, skinVar });
      if(ear!==null) earSeries.push(ear); if(mar!==null) marSeries.push(mar);
    }

    // Temporal features
    const meanY = per.map(p=>p.meanY);
    const block = per.map(p=>p.blockiness);
    const sharp = per.map(p=>p.lapEnergy);

    // Motion using blurred diffs
    const motion = temporalMotion(frames);

    // Blink rate (per minute) from EAR
    const blink = blinkStats(earSeries, fps);

    // Audio–lip sync correlation (if audio exists)
    const lipsync = await lipsyncCorrelation(times, marSeries, audioBuffer);

    // Filename risk cues
    const nameRisk = filenameRisk(currentFile?.name || '');

    // Aggregate metrics 0..1
    const flickerRisk = scaleU(stdDev(diff(meanY)), 1.2, 4.0);
    const blockinessRisk = uShape(mean(block), 0.6, 2.0);
    const sharpnessRisk = uShape(mean(sharp), 15, 45);
    const blinkRisk = blink ? blink.risk01 : 0.3; // neutral if not available
    const lipsyncRisk = lipsync?.risk01 ?? 0.3;
    const dctRisk = clamp(mean(per.map(p=>p.dctHF)),0,1);
    const skinRisk = clamp(mean(per.map(p=>p.skinVar)),0,1);
    const onnxProb = mean(per.filter(p=>p.onnxProb!=null).map(p=>p.onnxProb)) || null;

    const metrics = [
      {key:'onnx',   name:'Modelo IA (ONNX)',    val: onnxProb==null?0:onnxProb},
      {key:'blink',  name:'Parpadeo anómalo',    val: blinkRisk},
      {key:'lips',   name:'Labios vs audio',     val: lipsyncRisk},
      {key:'dct',    name:'Alta frecuencia DCT', val: dctRisk},
      {key:'skin',   name:'Consistencia de piel',val: skinRisk},
      {key:'block',  name:'Bloques compresión',  val: blockinessRisk},
      {key:'sharp',  name:'Nitidez anómala',     val: sharpnessRisk},
      {key:'name',   name:'Pistas del nombre',   val: nameRisk}
    ];

    // Weights (ajustables). Damos más peso al modelo si está disponible.
    const w = onnxProb==null
      ? { onnx:0, blink:.16, lips:.16, dct:.16, skin:.14, block:.14, sharp:.14, name:.10 }
      : { onnx:.55, blink:.10, lips:.12, dct:.08, skin:.07, block:.04, sharp:.04, name:.00 };

    const score01 = clamp(metrics.reduce((acc,m)=> acc + m.val*(w[m.key]||0), 0), 0, 1);
    const scorePct = Math.round(score01*100);
    const thr = parseInt(thres.value,10);
    const label = scorePct < (thr-10) ? 'Baja sospecha' : scorePct < (thr+10) ? 'Incierto' : 'Alta sospecha';

    const detail = metrics.map(m=>({ key:m.key, name:m.name, risk01:+m.val.toFixed(3)}));

    return {
      summary: { scorePct, label, frames: frames.length, fps, duration:+duration.toFixed(1), threshold: thr, onnx: onnxProb!=null, faceLandmarks: faceReady },
      metrics: detail,
      blink: blink || null,
      lipsync: lipsync || null,
      name: { file: currentFile?.name || null, risk01: nameRisk },
      tech: {
        means: { meanY:+mean(meanY).toFixed(2), block:+mean(block).toFixed(3), sharp:+mean(sharp).toFixed(2) },
        std:   { meanY:+stdDev(meanY).toFixed(3) }
      }
    };
  }

  async function ensureMetadata(video){
    return new Promise((res, rej)=>{
      if(isFinite(video.duration) && video.duration>0) return res();
      const onLoaded=()=>{ cleanup(); res(); }; const onErr=()=>{ cleanup(); rej(new Error('No se pudo leer metadatos')); };
      function cleanup(){ video.removeEventListener('loadedmetadata', onLoaded); video.removeEventListener('error', onErr); }
      video.addEventListener('loadedmetadata', onLoaded); video.addEventListener('error', onErr); if(video.readyState>=1) onLoaded();
    });
  }

  async function captureFrames(video, fps, maxFrames, capDuration, onProgress){
    const frames=[]; const times=[]; work.width=320; work.height=Math.round(320/(video.videoWidth/video.videoHeight || (16/9)));
    const start=0, end=Math.min(capDuration, video.duration||capDuration); const step=1/fps; const total=Math.min(Math.ceil((end-start)*fps), maxFrames);
    for(let i=0;i<total;i++){
      const t = start + i*step; if(t>end) break; const target = Math.min(t, end-0.001);
      video.currentTime = target; await once(video,'seeked'); wctx.drawImage(video,0,0,work.width,work.height);
      frames.push(wctx.getImageData(0,0,work.width,work.height)); times.push(target);
      if(onProgress) onProgress(Math.round((frames.length/total)*100));
    }
    return {frames, times};
  }

  // =============== Frame-level features ===============
  function frameStats(img){
    const { data, width:w, height:h } = img; const n=w*h; let sumY=0; let lapEnergy=0; let blockiness=0; const k=[-1,-1,-1,-1,8,-1,-1,-1,-1];
    const gray = new Float32Array(n);
    for(let i=0,j=0;i<data.length;i+=4,j++){ const r=data[i],g=data[i+1],b=data[i+2]; const y=0.299*r+0.587*g+0.114*b; sumY+=y; gray[j]=y; }
    const W=w,H=h, idx=(x,y)=>y*W+x;
    for(let y=1;y<H-1;y++){
      for(let x=1;x<W-1;x++){
        const g00=gray[idx(x-1,y-1)], g01=gray[idx(x,y-1)], g02=gray[idx(x+1,y-1)];
        const g10=gray[idx(x-1,y)],   g11=gray[idx(x,y)],   g12=gray[idx(x+1,y)];
        const g20=gray[idx(x-1,y+1)], g21=gray[idx(x,y+1)], g22=gray[idx(x+1,y+1)];
        const conv = k[0]*g00+k[1]*g01+k[2]*g02+k[3]*g10+k[4]*g11+k[5]*g12+k[6]*g20+k[7]*g21+k[8]*g22; lapEnergy += Math.abs(conv);
        if(x%8===0) blockiness += Math.abs(g11-g12); if(y%8===0) blockiness += Math.abs(g11-g21);
      }
    }
    const area=(W-2)*(H-2); lapEnergy/=area; blockiness/=area; const meanY=sumY/n; return { meanY, lapEnergy, blockiness };
  }

  function detectFace(img){
    if(!faceLandmarker) return null;
    // Draw image data to an offscreen canvas element to use with MediaPipe
    const tmp = document.createElement('canvas'); tmp.width = img.width; tmp.height = img.height; const tctx = tmp.getContext('2d'); tctx.putImageData(img,0,0);
    const res = faceLandmarker.detect(tmp);
    if(!res || !res.faceLandmarks || !res.faceLandmarks[0]) return null;
    const lm = res.faceLandmarks[0];
    // Compute bbox
    let minX=1e9,minY=1e9,maxX=-1e9,maxY=-1e9; for(const p of lm){ const x=p.x*img.width, y=p.y*img.height; if(x<minX)minX=x; if(y<minY)minY=y; if(x>maxX)maxX=x; if(y>maxY)maxY=y; }
    // Expand bbox
    const cx=(minX+maxX)/2, cy=(minY+maxY)/2; let w=maxX-minX, h=maxY-minY; const d=Math.max(w,h)*CFG.faceCropScale; const x0=Math.max(0, Math.round(cx-d/2)), y0=Math.max(0, Math.round(cy-d/2)); const x1=Math.min(img.width, Math.round(cx+d/2)), y1=Math.min(img.height, Math.round(cy+d/2));
    // Extract face patch 224x224
    const faceW = x1-x0, faceH=y1-y0; const faceImg = new ImageData(faceW, faceH); // copy pixels
    for(let y=0;y<faceH;y++){
      for(let x=0;x<faceW;x++){
        const srcIdx = ((y0+y)*img.width + (x0+x)) * 4; const dstIdx = (y*faceW + x) * 4; faceImg.data[dstIdx]=img.data[srcIdx]; faceImg.data[dstIdx+1]=img.data[srcIdx+1]; faceImg.data[dstIdx+2]=img.data[srcIdx+2]; faceImg.data[dstIdx+3]=255;
      }
    }
    fctx.clearRect(0,0,faceC.width,faceC.height); // draw and resize to 224
    const tmp2 = document.createElement('canvas'); tmp2.width=faceW; tmp2.height=faceH; const c2=tmp2.getContext('2d'); c2.putImageData(faceImg,0,0); fctx.drawImage(tmp2,0,0,faceC.width,faceC.height);

    // EAR & MAR from landmarks (using MP indexes)
    // Eye aspect ratio: use 33, 160, 158, 133, 153, 144 (approx for right eye) and mirror for left
    const idxsR = [33,160,158,133,153,144]; const idxsL = [362,385,387,263,373,380];
    const earR = eyeAspectRatio(lm, idxsR, img); const earL = eyeAspectRatio(lm, idxsL, img); const ear = (earR+earL)/2;
    // Mouth aspect ratio: use outer: 78 (left), 308 (right), 13 (top), 14 (bottom)
    const mar = mouthAspectRatio(lm, {L:78, R:308, T:13, B:14}, img);

    return { patch: fctx.getImageData(0,0,faceC.width,faceC.height), ear, mar };
  }

  function eyeAspectRatio(lm, idxs, img){
    const pts = idxs.map(i=>({x: lm[i].x*img.width, y: lm[i].y*img.height}));
    const A = dist(pts[1], pts[5]); const B = dist(pts[2], pts[4]); const C = dist(pts[0], pts[3]);
    return (A+B) / (2*C + 1e-6);
  }
  function mouthAspectRatio(lm, m, img){
    const L = {x: lm[m.L].x*img.width, y: lm[m.L].y*img.height};
    const R = {x: lm[m.R].x*img.width, y: lm[m.R].y*img.height};
    const T = {x: lm[m.T].x*img.width, y: lm[m.T].y*img.height};
    const B = {x: lm[m.B].x*img.width, y: lm[m.B].y*img.height};
    const horiz = dist(L,R); const vert = dist(T,B);
    return vert/(horiz+1e-6);
  }
  const dist = (a,b)=> Math.hypot(a.x-b.x, a.y-b.y);

  async function runOnnx(patch){
    if(!session) return null;
    // Preprocess 224x224 RGBA -> NCHW float32 0..1
    const {data,width,height} = patch; const N=1,C=3,H=224,W=224; const input = new Float32Array(N*C*H*W);
    let p=0; for(let y=0;y<H;y++){
      for(let x=0;x<W;x++){
        const idx = (y*width + x) * 4; const r=data[idx]/255, g=data[idx+1]/255, b=data[idx+2]/255;
        input[p++] = r; // R plane first
      }
    }
    for(let y=0;y<H;y++) for(let x=0;x<W;x++){ const idx=(y*width + x)*4; input[p++] = patch.data[idx+1]/255; }
    for(let y=0;y<H;y++) for(let x=0;x<W;x++){ const idx=(y*width + x)*4; input[p++] = patch.data[idx+2]/255; }

    const feeds = {}; feeds[session.inputNames[0]] = new ort.Tensor('float32', input, [1,3,224,224]);
    try{
      const out = await session.run(feeds); const key = session.outputNames[0]; let val = out[key].data[0];
      // If model outputs logits, apply sigmoid
      if(!isFinite(val) || Math.abs(val)>1){ val = 1/(1+Math.exp(-val)); }
      return clamp(val,0,1);
    }catch{ return null; }
  }

  function dctHighFreq(patch){
    // Compute grayscale 64x64 and 2D DCT, return energy in high frequencies proportion 0..1
    const S=64; const c=document.createElement('canvas'); c.width=S; c.height=S; const ctx=c.getContext('2d'); const tmp=document.createElement('canvas'); tmp.width=patch.width; tmp.height=patch.height; tmp.getContext('2d').putImageData(patch,0,0); ctx.drawImage(tmp,0,0,S,S); const img=ctx.getImageData(0,0,S,S).data;
    const g=new Float32Array(S*S); for(let i=0,j=0;i<img.length;i+=4,j++){ g[j]=0.299*img[i]+0.587*img[i+1]+0.114*img[i+2]; }
    // naive DCT-II
    const C = new Float32Array(S*S);
    const PI=Math.PI; const alpha = (u)=> u===0? Math.SQRT1_2 : 1;
    for(let u=0;u<S;u++){
      for(let v=0;v<S;v++){
        let sum=0; for(let x=0;x<S;x++) for(let y=0;y<S;y++){ sum += g[y*S+x]*Math.cos(((2*x+1)*u*PI)/(2*S))*Math.cos(((2*y+1)*v*PI)/(2*S)); }
        C[v*S+u] = 0.25*alpha(u)*alpha(v)*sum;
      }
    }
    let total=0, high=0; const cutoff=16; for(let v=0; v<S; v++) for(let u=0; u<S; u++){ const e=C[v*S+u]*C[v*S+u]; total+=e; if(u+v>cutoff) high+=e; }
    return clamp(high/(total+1e-6), 0, 1);
  }

  function skinChromVar(patch){
    // Compute variance of chrominance over frames would be better; here per-frame risk via intra-face variance proxy
    const {data,width,height}=patch; const n=width*height; let sumU=0,sumV=0; const U=new Float32Array(n), V=new Float32Array(n);
    for(let i=0,j=0;i<data.length;i+=4,j++){
      const r=data[i]/255, g=data[i+1]/255, b=data[i+2]/255; // simple YUV
      const y=0.299*r+0.587*g+0.114*b; const u=0.492*(b - y); const v=0.877*(r - y); U[j]=u; V[j]=v; sumU+=u; sumV+=v;
    }
    const mU=sumU/n, mV=sumV/n; let sU=0,sV=0; for(let i=0;i<n;i++){ sU+=(U[i]-mU)**2; sV+=(V[i]-mV)**2; }
    // Higher chroma variance can be suspicious (make it 0..1 via scale)
    return clamp((Math.sqrt(sU/n)+Math.sqrt(sV/n)) / 0.35, 0, 1);
  }

  function temporalMotion(frames){
    const out=[]; blurC.width=160; blurC.height=Math.round(160/(frames[0].width/frames[0].height)); const ctx= bctx; const tmp=document.createElement('canvas'); tmp.width=frames[0].width; tmp.height=frames[0].height; const tctx=tmp.getContext('2d');
    for(let i=1;i<frames.length;i++){
      const a=frames[i-1], b=frames[i]; tctx.putImageData(a,0,0); ctx.filter='blur(1.5px)'; ctx.drawImage(tmp,0,0,blurC.width,blurC.height); const A=ctx.getImageData(0,0,blurC.width,blurC.height).data;
      tctx.putImageData(b,0,0); ctx.filter='blur(1.5px)'; ctx.drawImage(tmp,0,0,blurC.width,blurC.height); const B=ctx.getImageData(0,0,blurC.width,blurC.height).data; let s=0,n=0; for(let k=0;k<A.length;k+=4){ const y1=0.299*A[k]+0.587*A[k+1]+0.114*A[k+2]; const y2=0.299*B[k]+0.587*B[k+1]+0.114*B[k+2]; s+=Math.abs(y1-y2); n++; } out.push(s/n);
    }
    ctx.filter='none'; return out;
  }

  // =============== Blink & lipsync ===============
  function blinkStats(earSeries, fps){
    if(!earSeries || earSeries.length<3) return null; const thr=0.21; let blinks=0, lastLow=false; for(const v of earSeries){ const low=v<thr; if(low && !lastLow) blinks++; lastLow=low; }
    const durationMin = earSeries.length / fps / 60; const rate = blinks / Math.max(0.01, durationMin);
    // Natural blink rate ~ 6–30/min; very bajo o nulo es sospechoso
    let risk = 0; if(rate<3) risk = 0.9; else if(rate<6) risk=0.6; else if(rate>35) risk=0.6; else risk=0.2; return { blinks, ratePerMin: +rate.toFixed(1), risk01: clamp(risk,0,1) };
  }

  async function lipsyncCorrelation(times, marSeries, audioBuffer){
    if(!audioBuffer || !marSeries || marSeries.length<6) return null;
    // Sample RMS audio around each frame time (±40ms)
    const sr = audioBuffer.sampleRate; const ch = audioBuffer.getChannelData(0); const win = Math.floor(0.04*sr);
    const rms=[]; for(const t of times){ const idx=Math.floor(t*sr); let s=0,n=0; for(let i=idx-win;i<=idx+win;i++){ if(i>=0 && i<ch.length){ const v=ch[i]; s+=v*v; n++; } } rms.push(Math.sqrt(s/Math.max(1,n))); }
    // Normalize sequences
    const a = zscore(marSeries); const b = zscore(rms);
    const corr = pearson(a,b);
    // If there is speech energy but poor lipsync, flag
    const speechEnergy = mean(rms);
    const risk = speechEnergy>0.02 ? clamp(0.7 - corr*0.7, 0, 1) : 0.3; // worse corr => higher risk
    return { corr:+corr.toFixed(2), energy:+speechEnergy.toFixed(3), risk01:risk };
  }

  // =============== Helpers ===============
  const mean = arr => arr.reduce((a,b)=>a+b,0)/arr.length;
  const diff = arr => arr.slice(1).map((v,i)=> v - arr[i]);
  const stdDev = arr => { const m=mean(arr); let s=0; for(const v of arr){ s+=(v-m)*(v-m); } return Math.sqrt(s/arr.length); }
  const clamp = (x,a,b)=> Math.max(a, Math.min(b, x));
  const scale = (v, lo, hi)=> clamp((v - lo) / (hi - lo), 0, 1);
  const uShape = (v, lo, hi)=>{ if(hi<=lo) return 0; const mid=(lo+hi)/2; const span=(hi-lo)/2; const d=Math.abs(v-mid)/span; return clamp(d,0,1); };
  const scaleU = (v, lo, hi)=>{ const t = scale(v, lo, hi); return Math.pow(t,0.85); };
  const zscore = arr => { const m=mean(arr), s=stdDev(arr)||1; return arr.map(v=>(v-m)/s); };
  const pearson = (a,b)=>{ const ma=mean(a), mb=mean(b); let num=0,da=0,db=0; for(let i=0;i<a.length;i++){ const x=a[i]-ma, y=b[i]-mb; num+=x*y; da+=x*x; db+=y*y; } return num/Math.sqrt((da||1)*(db||1)); };

  // =============== Render ===============
  function renderReport(rep){ setGauge(rep.summary.scorePct, rep.summary.label); legendEl.innerHTML=''; rep.metrics.forEach(m=> legendEl.appendChild(metricRow(m.name, Math.round(m.risk01*100)))); }
  function metricRow(name, pct){ const row=document.createElement('div'); row.className='metric-row'; const n=document.createElement('div'); n.className='name'; n.textContent=name; const bar=document.createElement('div'); bar.className='bar'; const fill=document.createElement('span'); fill.style.width=pct+'%'; bar.appendChild(fill); const v=document.createElement('div'); v.className='val'; v.textContent=pct+'%'; row.append(n,bar,v); return row; }
  function setGauge(pct,label){ pct=Math.round(pct); drawGauge(gaugeCanvas,pct); pctEl.textContent=isFinite(pct)? pct+'%':'—%'; labelEl.textContent=label||'—'; }
  function drawGauge(canvas,pct){ const ctx=canvas.getContext('2d'); const w=canvas.width,h=canvas.height,r=w/2-8,cx=w/2,cy=h/2; ctx.clearRect(0,0,w,h); ctx.lineWidth=12; ctx.strokeStyle='rgba(255,255,255,.12)'; ctx.beginPath(); ctx.arc(cx,cy,r, Math.PI*0.75, Math.PI*2.25); ctx.stroke(); const t=pct/100; const start=Math.PI*0.75; const end=start + t*(Math.PI*1.5); const grad=ctx.createLinearGradient(0,0,w,0); grad.addColorStop(0,'#7aa2ff'); grad.addColorStop(1,'#8af0d6'); ctx.strokeStyle=grad; ctx.lineWidth=12; ctx.lineCap='round'; ctx.beginPath(); ctx.arc(cx,cy,r,start,end); ctx.stroke(); }

  // =============== Filename risk cues ===============
  function filenameRisk(name){
    if(!name) return 0; const low=name.toLowerCase(); const hints=['deepfake','faceswap','fsgan','roop','sdxl','synth','ai','gen','xseg','dfaker','vid2vid','gfpgan'];
    let score=0; for(const h of hints){ if(low.includes(h)) score+=0.2; } if(/[0-9]{3,4}x[0-9]{3,4}/.test(low)) score+=0.05; return clamp(score,0,1);
  }
  </script>
</body>
</html>