# Informe T√©cnico ‚Äî Detector de Deepfakes (Versi√≥n Pro Local)

Este informe documenta en detalle el funcionamiento del sistema de detecci√≥n de deepfakes implementado en un archivo **HTML + CSS + JavaScript**. Su objetivo es permitir a un lector (profesor o revisor t√©cnico) comprender **qu√© hace el sistema, c√≥mo se calculan las m√©tricas, c√≥mo est√°n organizados los m√©todos/clases y c√≥mo se integrar√≠an los modelos IA externos (ONNX/MediaPipe)**.

---

## 1. Introducci√≥n

El sistema es un **detector de deepfakes local** que corre 100% en el navegador. No necesita backend, y analiza un video cargado por el usuario mediante:

1. Extracci√≥n de frames (hasta 5 fps).
2. Procesamiento de cada frame para calcular m√©tricas visuales.
3. Opcional: detecci√≥n de rostro y clasificaci√≥n con un modelo IA en formato ONNX.
4. An√°lisis del audio (si existe) para comprobar sincron√≠a labios‚Äìvoz.
5. An√°lisis del nombre del archivo para pistas contextuales.
6. Integraci√≥n de todas las m√©tricas en un **score final 0‚Äì100%**.

El usuario recibe:

* Un **gauge circular** con porcentaje de sospecha.
* **Barras por m√©trica**, con tooltips explicativos.
* La opci√≥n de **exportar un reporte JSON**.

---

## 2. Flujo general del sistema

### 2.1 Entrada

* **Archivo de video** (`.mp4/.mov/.webm`).
* **Nombre de archivo** (string).

### 2.2 Procesamiento

* Conversi√≥n del video a frames en un `<canvas>` oculto.
* Preprocesamiento de audio con `AudioContext`.
* Ejecuci√≥n de m√©tricas (visuales, temporales y de audio).
* Combinaci√≥n ponderada de m√©tricas en un score final.

### 2.3 Salida

* Probabilidad de manipulaci√≥n (0‚Äì100%).
* Estado interpretado (baja sospecha, incierto, alta sospecha).
* Reporte JSON opcional con desglose t√©cnico.

---

## 3. Arquitectura del c√≥digo

### 3.1 Componentes principales

* **UI principal (HTML+CSS)**: interfaz de carga, controles, gauge y resultados.
* **M√≥dulo de captura de frames (`captureFrames`)**: obtiene `ImageData` de cada frame.
* **Funciones de m√©tricas**: calculan valores espec√≠ficos de cada frame o secuencia.
* **Agregador de m√©tricas (`analyzeVideo`)**: integra resultados y genera un reporte.
* **Renderizador (`renderReport`)**: dibuja gauge y barras de m√©tricas.

### 3.2 Clases y m√©todos clave

* `frameStats(img)`: devuelve luminancia media, nitidez (Laplaciano) y blockiness.
* `dctHighFreq(img)`: calcula energ√≠a en altas frecuencias via DCT.
* `skinChromVar(patch)`: eval√∫a variabilidad crom√°tica en la piel.
* `temporalMotion(frames)`: mide inestabilidad del movimiento entre frames.
* `blinkStats(earSeries, fps)`: estima frecuencia de parpadeo a partir de EAR.
* `lipsyncCorrelation(times, marSeries, audioBuffer)`: correlaciona energ√≠a de audio con actividad oral.
* `filenameRisk(name)`: busca palabras clave sospechosas en el nombre del archivo.

---

## 4. Explicaci√≥n de m√©tricas

### 4.1 Flicker de brillo

* **M√©todo:** `stdDev(diff(meanY))` sobre luminancia media entre frames.
* **Rango esperado:** estable en videos reales, alto en deepfakes.
* **Motivo:** parpadeos de iluminaci√≥n suelen ser artefactos de generaci√≥n.

### 4.2 Alta frecuencia DCT

* **M√©todo:** DCT 2D de un recorte central 64√ó64; se calcula proporci√≥n de energ√≠a en frecuencias altas.
* **Rango esperado:** natural \~0.2‚Äì0.4; deepfakes pueden subir a >0.6.
* **Motivo:** s√≠ntesis genera texturas con exceso de detalle artificial.

### 4.3 Inestabilidad de movimiento

* **M√©todo:** diferencias absolutas en frames consecutivos tras aplicar blur.
* **Rango esperado:** suave en reales; err√°tico en falsos.
* **Motivo:** warping/re-iluminaci√≥n inconsistente.

### 4.4 Bloques de compresi√≥n

* **M√©todo:** variaciones en bordes de bloques 8√ó8.
* **Rango esperado:** uniforme en videos reales; elevado en falsos.
* **Motivo:** regiones manipuladas muestran distinta compresi√≥n que el resto.

### 4.5 Nitidez an√≥mala

* **M√©todo:** energ√≠a Laplaciana; se penalizan valores muy bajos (desenfoque) o muy altos (oversharpen).
* **Motivo:** blending y escalado alteran nitidez natural.

### 4.6 Labios vs audio

* **M√©todo:** correlaci√≥n Pearson entre RMS de audio y serie MAR (apertura de boca).
* **Rango esperado:** >0.5 en reales; <0.3 en falsos.
* **Motivo:** deepfakes suelen desincronizar labios y voz.

### 4.7 Consistencia de piel

* **M√©todo:** varianza de crominancia U/V.
* **Motivo:** regiones sint√©ticas presentan inconsistencias de tono.

### 4.8 Pistas del nombre

* **M√©todo:** regex + keywords ("deepfake", "faceswap", etc.).
* **Motivo:** ayuda a sumar contexto de riesgo.

## üîπ C√≥mo est√° hecho el modelo (parte IA)

El sistema soporta dos formas de ‚Äúmodelo‚Äù:

1. **Heur√≠stico puro (sin IA)**

   * Funciona siempre, solo con **JavaScript + Canvas** en el navegador.
   * Se basa en **m√©tricas num√©ricas** calculadas sobre los frames y el audio (si lo hay).
   * Ventaja: corre offline, r√°pido y sin dependencias.
   * Limitaci√≥n: menos preciso ante deepfakes muy sofisticados.

2. **Modelo IA en ONNX (opcional)**

   * Se carga un modelo ligero exportado a **ONNX** (por ej. XceptionNet entrenado en FaceForensics++).
   * Pipeline:

     1. Se detecta y recorta la **cara** (224√ó224 px).
     2. Se normaliza a un tensor `Float32` en formato `[1, 3, 224, 224]`.
     3. Se pasa al **ONNX Runtime Web** en el navegador.
     4. El modelo devuelve un **logit o probabilidad** ‚Üí se aplica `sigmoid` si hace falta.
     5. Ese valor (0‚Äì1) se integra en el **score global** con m√°s peso que el resto de m√©tricas.

üëâ En tu versi√≥n final, si no hay modelo cargado, el sistema sigue en modo heur√≠stico. Si hay ONNX v√°lido, ese valor domina el resultado (‚âà55% del peso).

---

## üîπ C√≥mo se mide cada m√©trica (heur√≠sticas)

### 1. Flicker de brillo

* **C√≥mo:** se calcula la luminancia media (`meanY`) de cada frame y se mide la desviaci√≥n est√°ndar entre diferencias sucesivas (`stdDev(diff(meanY))`).
* **Qu√© indica:** parpadeos o variaciones de luz anormales.

### 2. Alta frecuencia DCT

* **C√≥mo:** se hace un **DCT 2D** de un recorte central 64√ó64 en escala de grises.
* Se mide la proporci√≥n de energ√≠a en frecuencias altas (`high / total`).
* **Qu√© indica:** texturas artificiales o exceso de detalle.

### 3. Inestabilidad de movimiento

* **C√≥mo:** se difuminan dos frames consecutivos y se calcula la diferencia promedio en escala de grises.
* **Qu√© indica:** saltos o warping entre cuadros.

### 4. Bloques de compresi√≥n

* **C√≥mo:** se mide la diferencia de luminancia en bordes de bloques 8√ó8.
* **Qu√© indica:** compresi√≥n desigual entre cara y fondo.

### 5. Nitidez an√≥mala

* **C√≥mo:** se aplica un filtro Laplaciano y se mide su energ√≠a.
* **Qu√© indica:** excesivo sharpening o desenfoque raro por mezcla de capas.

### 6. Consistencia de piel

* **C√≥mo:** se convierte a espacio YUV y se mide la varianza de crominancia U/V en la cara.
* **Qu√© indica:** variaciones de tono no naturales en piel generada.

### 7. Parpadeo (EAR)

* **C√≥mo:** con landmarks faciales (MediaPipe) se mide el **Eye Aspect Ratio**.
* Se cuentan parpadeos/minuto y se compara con lo fisiol√≥gico (6‚Äì30/min).
* **Qu√© indica:** pocos parpadeos = sospecha.

### 8. Labios vs audio

* **C√≥mo:** se mide el **Mouth Aspect Ratio (MAR)** y se correlaciona con la energ√≠a RMS del audio.
* **Qu√© indica:** baja correlaci√≥n cuando hay voz = lipsync falso.

### 9. Pistas del nombre

* **C√≥mo:** regex + keywords en el nombre del archivo (`deepfake`, `faceswap`, `ai`, etc.).
* **Qu√© indica:** metadatos sospechosos.

---

üìä **Score final:**

$$
\\text{Score} = \\sum (\\text{m√©trica} \\times \\text{peso})
$$

* Sin IA: pesos distribuidos (‚âà10‚Äì18% cada m√©trica).
* Con IA: el modelo ONNX pesa ‚âà55%, y las dem√°s ajustan el resto.


## 5. Modelos IA (opcionales)

### 5.1 ONNX (Xception/EfficientNet preentrenado)

* Se recorta la cara (224√ó224), se normaliza a tensor `[1,3,224,224]`.
* Se pasa al `InferenceSession` de ONNX Runtime Web.
* Se aplica `sigmoid` si la salida es logit.
* El valor resultante (0‚Äì1) se pondera fuertemente en el score.

### 5.2 MediaPipe Face Landmarker

* Detecta landmarks (ojos, boca, contorno facial).
* Calcula EAR (Eye Aspect Ratio) y MAR (Mouth Aspect Ratio).
* Permite parpadeo y lipsync m√°s precisos.

---

## 6. C√°lculo del score final

### 6.1 F√≥rmula

```js
score01 = Œ£ (m√©trica.valor √ó peso)
```

* Pesos ajustables seg√∫n disponibilidad de ONNX/landmarks.
* Ejemplo sin IA: cada m√©trica pesa entre 10‚Äì18%.
* Ejemplo con IA: ONNX pesa \~55%.

### 6.2 Interpretaci√≥n

* **0‚Äì35%:** aut√©ntico probable.
* **35‚Äì65%:** incierto (revisi√≥n manual).
* **65‚Äì100%:** potencial deepfake.

---

## 7. Limitaciones

* Dependencia de calidad: videos muy comprimidos pueden falsear m√©tricas.
* M√©tricas heur√≠sticas son aproximadas.
* Modelo ONNX recomendado para reforzar an√°lisis.
* Audio opcional: no siempre disponible.

---

## 8. Posibles mejoras

* Integrar Grad-CAM para visualizar zonas sospechosas.
* Guardar thumbnails de frames sospechosos.
* Ajustar pesos din√°micamente seg√∫n dataset.
* Integrar un backend (FastAPI + PyTorch) para modelos m√°s pesados.

---

## 9. Conclusiones

El sistema combina **m√©tricas heur√≠sticas** con la posibilidad de usar **modelos preentrenados** para ofrecer un detector flexible y educativo. Es un prototipo √∫til para comprender los riesgos de los deepfakes y los m√©todos de detecci√≥n.
